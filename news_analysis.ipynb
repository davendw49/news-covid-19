{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import torch\n",
    "from flair.embeddings import FlairEmbeddings, DocumentPoolEmbeddings, Sentence, BertEmbeddings,WordEmbeddings\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial import distance_matrix\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "import random\n",
    "import seaborn as sns\n",
    "import chartify\n",
    "import os\n",
    "matplotlib.use('AGG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class doument_analysis:\n",
    "    def __init__(self,path_csv):\n",
    "        self.path = path_csv\n",
    "        self.df = pd.read_csv(path_csv)\n",
    "        self.filepath,self.fullname=os.path.split(path_csv)\n",
    "        self.fname,self.ext=os.path.splitext(self.fullname)\n",
    "        self.N_CLUSTERS = 10\n",
    "    def csv_token(self):#中文分词\n",
    "        stopword =[]\n",
    "        with open('./stopwords.txt','r') as st:\n",
    "            for line in st:\n",
    "                stopword.append(line.strip())\n",
    "        def cut(x):\n",
    "            segs = []\n",
    "            for word in jieba.cut(x):\n",
    "                if word not in stopword:\n",
    "                    segs.append(word)\n",
    "            return ' '.join(segs)\n",
    "        self.df['正文']= self.df['正文'].apply(cut)\n",
    "        self.df.to_csv(self.filepath+'/'+self.fname+\"_token\"+'.csv')\n",
    "    \n",
    "    def document_embedding(self):\n",
    "        #self.df=pd.read_csv(self.filepath+'/'+self.fname+\"_token\"+\".csv\") #有分好词的csv的时候直接取消注释\n",
    "        flair_embedding_forward = FlairEmbeddings('./news-forward-0.4.1.pt')\n",
    "        flair_embedding_backward = FlairEmbeddings('./news-backward-0.4.1.pt')\n",
    "        #bert_embedding = BertEmbeddings('bert-base-chinese')\n",
    "        glove_embedding = WordEmbeddings('./glove.gensim')\n",
    "        # combine word embedding models\n",
    "        document_embeddings = DocumentPoolEmbeddings([glove_embedding, flair_embedding_backward, flair_embedding_forward])\n",
    "        CUDA_LAUNCH_BLOCKING=1 \n",
    "        # set up empty tensor\n",
    "        X = torch.empty(size=(len(self.df.index),4196)).cuda()\n",
    "        #X = torch.empty(size=(len(self.df.index)//3,4196)).cuda()\n",
    "        #X = torch.empty(size=(len(self.df.index)-len(self.df.index)//3,4196)).cuda()\n",
    "        #X = torch.empty(size=(len(self.df.index)-len(self.df.index)//3*2,4196)).cuda()\n",
    "        #torch.backends.cudnn.enabled = False\n",
    "        # fill tensor with embeddings\n",
    "        i=0\n",
    "        #for text in tqdm(self.df['正文'][:len(self.df.index)//3]):\n",
    "        #for text in tqdm(self.df['正文'][len(self.df.index)//3:len(self.df.index)//3*2]):\n",
    "        #for text in tqdm(self.df['正文'][len(self.df.index)//3*2:]):\n",
    "        for text in tqdm(self.df['正文']):\n",
    "            sentence = Sentence(text)\n",
    "            document_embeddings.embed(sentence)\n",
    "            embedding = sentence.get_embedding()\n",
    "            X[i] = embedding\n",
    "            i += 1\n",
    "            \n",
    "        self.X = X.cpu().detach().numpy()\n",
    "        np.save(self.filepath+'/'+self.fname+\"_doc_emd.npy\",self.X)\n",
    "        del(X)\n",
    "        \n",
    "    def PCA_cluster(self):\n",
    "        self.X=np.load(self.filepath+'/'+self.fname+\"_doc_emd.npy\")#有词向量打开\n",
    "        self.df=pd.read_csv(self.filepath+'/'+self.fname+\"_token\"+\".csv\")#有分好词的csv打开\n",
    "        pca =PCA(n_components=768)\n",
    "        self.X_red=pca.fit_transform(self.X)\n",
    "        #self.N_CLUSTERS = 10\n",
    "        ward = AgglomerativeClustering(n_clusters=self.N_CLUSTERS,affinity='euclidean',linkage='ward')\n",
    "        self.pred_ward = ward.fit_predict(self.X_red)\n",
    "        self.df['topic'] = self.pred_ward\n",
    "    def ploting_higram(self):\n",
    "        matplotlib.rcParams['axes.unicode_minus']=False     \n",
    "        plt.hist(self.df['topic'], bins=10, facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "        # 显示横轴标签\n",
    "        plt.xlabel(\"Interval\")\n",
    "        # 显示纵轴标签\n",
    "        plt.ylabel(\"Frequency/frequency\")\n",
    "        # 显示图标题\n",
    "        plt.title(\"Frequency/frequency distribution histogram\")\n",
    "        plt.savefig(self.filepath+'/'+self.fname+'_freout.png',dpi=300)\n",
    "        plt.tight_layout()\n",
    "        plt.cla()\n",
    "    \n",
    "    def ploting_scatter(self):\n",
    "        pca = PCA(n_components=2)\n",
    "        X_two = pca.fit_transform(self.X_red)\n",
    "        centers = defaultdict(list)\n",
    "        for label, location in zip(self.pred_ward,X_two):\n",
    "            centers[label].append(location)\n",
    "        color = ['red','green','grey','black','yellow','orange','pink','blue','brown','violet']\n",
    "        for i,c in enumerate(centers):\n",
    "            for location in centers[c]:\n",
    "                plt.scatter(*location,c=color[i])\n",
    "        plt.savefig(self.filepath+'/'+self.fname+'_scatter.png',dpi=300)\n",
    "        plt.tight_layout()\n",
    "        plt.cla()\n",
    "    def get_top_ten(self):\n",
    "\n",
    "        def get_top_words(documents, top_n):\n",
    "            '''\n",
    "            function to get top tf-idf words and phrases\n",
    "            '''\n",
    "            vectoriser = TfidfVectorizer(ngram_range=(1, 2),max_df=0.5)\n",
    "            tfidf_matrix = vectoriser.fit_transform(documents)\n",
    "            feature_names = vectoriser.get_feature_names()\n",
    "            df_tfidf = pd.DataFrame()\n",
    "            for doc in range(len(documents)):\n",
    "                words = []\n",
    "                scores = []\n",
    "                feature_index = tfidf_matrix[doc,:].nonzero()[1]\n",
    "                tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index])\n",
    "                for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "                    words.append(w)\n",
    "                    scores.append(s)\n",
    "                df_temp = pd.DataFrame(data={'word':words, 'score':scores})\n",
    "                df_temp = df_temp.sort_values('score',ascending=False).head(top_n)\n",
    "                df_temp['topic'] = doc\n",
    "                df_tfidf = df_tfidf.append(df_temp)\n",
    "            return df_tfidf\n",
    "\n",
    "        topic_docs = []\n",
    "        # group text into topic-documents\n",
    "        for topic in range(self.N_CLUSTERS):\n",
    "            topic_docs.append(' '.join(self.df[self.df['topic']==topic]['正文'].values))\n",
    "        # apply function\n",
    "        self.df_tfidf = get_top_words(topic_docs, 10)\n",
    "        self.df_tfidf.to_csv(self.filepath+'/'+self.fname+\"_top_ten\"+'.csv')\n",
    "    def ploting_ten(self):       \n",
    "        for i in range(10):\n",
    "            # 中文乱码和坐标轴负号处理。\n",
    "            #matplotlib.rc('font', family='SimHei', weight='bold')\n",
    "            \"\"\" matplotlib.rcdefaults()\n",
    "            plt.rcParams['axes.unicode_minus'] = False\n",
    "            plt.rcParams.update({'figure.autolayout': True})\n",
    "            # 实例字体对象\n",
    "            font = font_manager.FontProperties(fname=r'./simhei.ttf')\n",
    "            #城市数据。 \"\"\"\n",
    "            word = self.df_tfidf['word'].tolist()[i*10:(i+1)*10]\n",
    "            data = self.df_tfidf['score'].tolist()[i*10:(i+1)*10]\n",
    "            dic = {\n",
    "            'keywords':word,\n",
    "            'rate':data\n",
    "            }\n",
    "            topic=pd.DataFrame(dic)\n",
    "            ch = chartify.Chart(blank_labels=True, y_axis_type='categorical')\n",
    "            ch.set_title(\"Theme Horizontal bar plot\")\n",
    "            ch.plot.bar(\n",
    "                data_frame=topic,\n",
    "                categorical_columns='keywords',\n",
    "                numeric_column='rate',\n",
    "                categorical_order_by='values',\n",
    "                # color_column='rate'\n",
    "            )\n",
    "            #ch.show(self.filepath+'/'+self.fname+\"_top_ten\"+str(i)+\".png\")\n",
    "            ch.save(self.filepath+'/'+self.fname+\"_top_ten\"+str(i)+\".\"+\"html\")\n",
    "            \n",
    "    def ploting_gauss(self):\n",
    "        self.topic_centroids=[]\n",
    "        dic ={0:'first',1:'second',2:'third',3:'forth',4:'fifth',5:'sixth',6:'seventh',7:'eighth',8:'ninth',9:'tenth'}\n",
    "        for topic in tqdm(range(self.N_CLUSTERS)):\n",
    "            X_topic = self.X_red[self.df.index[self.df['topic']==topic]]\n",
    "            X_mean = np.mean(X_topic, axis=0)\n",
    "            self.topic_centroids.append(X_mean)\n",
    "        topic_distances = []\n",
    "        for row in tqdm(self.df.index):\n",
    "            topic_centroid = self.topic_centroids[self.df.iloc[row]['topic']]\n",
    "            X_row = self.X_red[row]\n",
    "            topic_distance = euclidean(topic_centroid, X_row)\n",
    "            topic_distances.append(topic_distance)\n",
    "            \n",
    "        self.df['topic_distance'] = topic_distances\n",
    "        fig, ax = plt.subplots(figsize = (10,10))\n",
    "        sns.kdeplot(self.df[self.df['topic']==0]['topic_distance'], shade=True,label=\"the zero theme\")\n",
    "        sns.kdeplot(self.df[self.df['topic']==1]['topic_distance'], shade=True,label=\"the first theme\")\n",
    "        sns.kdeplot(self.df[self.df['topic']==2]['topic_distance'], shade=True,label=\"the second theme\")\n",
    "        sns.kdeplot(self.df[self.df['topic']==3]['topic_distance'], shade=True,label=\"the third theme\")\n",
    "        sns.kdeplot(self.df[self.df['topic']==4]['topic_distance'], shade=True,label=\"the forth theme\")\n",
    "        sns.kdeplot(self.df[self.df['topic']==5]['topic_distance'], shade=True,label=\"the fifth theme\")\n",
    "        sns.kdeplot(self.df[self.df['topic']==6]['topic_distance'], shade=True,label=\"the sixth theme\")\n",
    "        sns.kdeplot(self.df[self.df['topic']==7]['topic_distance'], shade=True,label=\"the seven theme\")\n",
    "        sns.kdeplot(self.df[self.df['topic']==8]['topic_distance'], shade=True,label=\"the eight theme\")\n",
    "        kd=sns.kdeplot(self.df[self.df['topic']==9]['topic_distance'], shade=True,label=\"the ninth theme\")\n",
    "        fig =kd.get_figure()\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(self.filepath+'/'+self.fname+'_gauss.png',dpi=300)\n",
    "        plt.cla()\n",
    "        for i in range(10):\n",
    "            fig,ax=plt.subplots(figsize=(10,10))\n",
    "            kd=sns.kdeplot(self.df[self.df['topic']==i]['topic_distance'],shade=True,label=\"the \"+dic[i]+\" theme\")\n",
    "            fig=kd.get_figure()\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(self.filepath+'/'+self.fname+'_gauss'+str(i)+'.png',dpi=300)\n",
    "            plt.cla()\n",
    "    def ploting_matrix(self):\n",
    "\n",
    "        df_dist_matrix = pd.DataFrame(distance_matrix(self.topic_centroids,self.topic_centroids),index=range(self.N_CLUSTERS),columns=range(self.N_CLUSTERS))\n",
    "\n",
    "        a = np.array(df_dist_matrix)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (20,20))\n",
    "        #二维的数组的热力图，横轴和数轴的ticklabels要加上去的话，既可以通过将array转换成有column\n",
    "        #和index的DataFrame直接绘图生成，也可以后续再加上去。后面加上去的话，更灵活，包括可设置labels大小方向等。\n",
    "        sns.heatmap(pd.DataFrame(np.round(a,2), columns = ['theme0', 'theme1', 'theme2','theme3','theme4','theme5','theme6','theme7','theme8','theme9'], index = ['theme0', 'theme1', 'theme2','theme3','theme4','theme5','theme6','theme7','theme8','theme9']),\n",
    "                        annot=True, vmax=1,vmin = 0, xticklabels= True, yticklabels= True, square=True, cmap=\"Blues\")\n",
    "        #sns.heatmap(np.round(a,2), annot=True, vmax=1,vmin = 0, xticklabels= True, yticklabels= True,\n",
    "        #            square=True, cmap=\"YlGnBu\")\n",
    "        # ax.set_title('二维数组热力图', fontsize = 18)\n",
    "        ax.set_ylabel('image', fontsize = 18)\n",
    "        ax.set_xlabel('iamge', fontsize = 18) #横变成y轴，跟矩阵原始的布局情况是一样的\n",
    "        plt.savefig(self.filepath+'/'+self.fname+'_matrix.png',dpi=300)\n",
    "        print(self.filepath+'/'+self.fname+'_matrix.png')\n",
    "        plt.tight_layout()\n",
    "        plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./sample/news_202002_top_ten0.html\n",
      "Saved to ./sample/news_202002_top_ten1.html\n",
      "Saved to ./sample/news_202002_top_ten2.html\n",
      "Saved to ./sample/news_202002_top_ten3.html\n",
      "Saved to ./sample/news_202002_top_ten4.html\n",
      "Saved to ./sample/news_202002_top_ten5.html\n",
      "Saved to ./sample/news_202002_top_ten6.html\n",
      "Saved to ./sample/news_202002_top_ten7.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 421.23it/s]\n",
      "  0%|          | 0/10291 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./sample/news_202002_top_ten8.html\n",
      "Saved to ./sample/news_202002_top_ten9.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10291/10291 [00:04<00:00, 2223.61it/s]\n",
      "/home/daven/anaconda3/envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/daven/anaconda3/envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:218: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sample/news_202002_matrix.png\n"
     ]
    }
   ],
   "source": [
    "da=doument_analysis('./sample/news_202002.csv')\n",
    "da.csv_token()#分词\n",
    "# da.document_embedding()#embedding\n",
    "da.PCA_cluster()#PCA聚类\n",
    "da.ploting_higram()\n",
    "da.ploting_scatter()\n",
    "da.get_top_ten()\n",
    "da.ploting_ten()\n",
    "da.ploting_gauss()\n",
    "da.ploting_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
